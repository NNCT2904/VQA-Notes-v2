{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training Variational Quantum Models with Barren Plateaus Mitigation Strategies\n",
    "*Training QNN treated with method 3 (Identity Blocks) of the BP mitigation strategy (Cancer)*\n",
    "\n",
    "**Authors:**\n",
    "- Jacob Cybulski and Thanh Nguyen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.providers.fake_provider import FakeAlmadenV2, FakeSherbrooke\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "from qiskit.primitives import Estimator, BackendEstimator\n",
    "from qiskit_algorithms.gradients import ParamShiftEstimatorGradient\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from Components.train import *\n",
    "from Components.data import cancer_data\n",
    "from Components.circuits import *\n",
    "from Components.gradients import *\n",
    "from Components.utils import *\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "from os import makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global config for all experiments (data size, feature dim etc.) is stored here\n",
    "from GLOBAL_CONFIG import *\n",
    "# Remember to tag the method\n",
    "METHOD_TAG = 'm3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend = FakeSherbrooke()\n",
    "# estimator = BackendEstimator(backend)\n",
    "estimator = Estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = cancer_data(PCA_n = FEATURE_DIM)\n",
    "# X_train, X_val, y_train, y_val = fetch_mnist(PCA_n = FEATURE_DIM, data_size=DATA_SIZE)\n",
    "# X_train, X_val, y_train, y_val = iris(pd=False, PCA_n=None)\n",
    "\n",
    "X_train_t = torch.from_numpy(X_train).to(torch.float32)\n",
    "y_train_t = torch.from_numpy(y_train).to(torch.float32)\n",
    "X_val_t = torch.from_numpy(X_val).to(torch.float32)\n",
    "y_val_t = torch.from_numpy(y_val).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (exists(f'{LOG_PATH}/{METHOD_TAG}')):\n",
    "    print(f'Found save folder: {LOG_PATH}/{METHOD_TAG}\\n')\n",
    "else:\n",
    "    makedirs(f'{LOG_PATH}/{METHOD_TAG}')\n",
    "    print(f'Creating save folder: {LOG_PATH}/{METHOD_TAG}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Gradient Variance Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits = list(range(2, MAX_QUBITS))\n",
    "\n",
    "# Globak operator for all ansatzes, measure all qubits\n",
    "G_O = [SparsePauliOp.from_list([('Z'*n, 1)]) for n in num_qubits]\n",
    "\n",
    "# Local operator for all ansatzes, measere 2 last qubits\n",
    "L_O = [SparsePauliOp.from_list([('I' * (n - 2)+'Z'*2, 1)]) for n in num_qubits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleM3Var():\n",
    "# Generate the pre-trained circuits\n",
    "    pre_trained_block = [preTrainedBlockGenerator(n, int(np.floor(n/2))) for n in num_qubits]\n",
    "\n",
    "    ansatzes_m3 = []\n",
    "    for i in range(len(pre_trained_block)):\n",
    "        ansatzes_m3.append(pre_trained_block[i]['circuit'])\n",
    "\n",
    "    parameters_m3 = []\n",
    "    for i in range(len(pre_trained_block)):\n",
    "        parameters_m3.append(list(pre_trained_block[i]['params_values'].values()))\n",
    "\n",
    "    gradients_m0 = sampleAnsatz(estimator, ansatzes_m3, G_O, parameters_m3)\n",
    "\n",
    "    variance = getVariance(gradients_m0, num_qubits)\n",
    "    \n",
    "    return variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = featureMapGenerator(FEATURE_DIM)\n",
    "identity_block = preTrainedBlockGenerator(MAX_QUBITS_CLASSIFICATION, MAX_IDENTITIES_BLOCKS, overlay=IDENTITY_BLOCKS_OVERLAY, entanglement=ENTANGLEMENT, insert_barriers=True)\n",
    "ansatz = identity_block['circuit']\n",
    "qc=circuitBuilder(feature_map, ansatz)\n",
    "\n",
    "\n",
    "id_dict = {k.name : v for k, v in identity_block['params_values'].items()}\n",
    "initial_point = [id_dict[p.name] for p in list(ansatz.parameters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find what devices are available\n",
    "from qiskit_aer.backends import AerSimulator\n",
    "devices = AerSimulator().available_devices()\n",
    "devices\n",
    "\n",
    "# Force CPU - PyTorch+Qiskit too slow with GPU\n",
    "devices = ('CPU')\n",
    "devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.primitives import Estimator # For \"CPU\", ignores device=\"GPU\" option\n",
    "from qiskit_aer.primitives import Estimator as AerEstimator # For device=\"GPU\" option\n",
    "\n",
    "seed = 2023\n",
    "\n",
    "# Use GPU when present, otherwise CPU\n",
    "if 'GPU' in devices:\n",
    "    device = 'GPU'\n",
    "    estimator = AerEstimator(\n",
    "        backend_options={'seed_simulator': seed, 'method': 'statevector', \n",
    "                         'device' : device, 'cuStateVec_enable' : True},\n",
    "        run_options={'seed': seed, 'shots': 1000},\n",
    "        transpile_options={\"seed_transpiler\": seed},\n",
    "    )\n",
    "else:\n",
    "    device = 'CPU'\n",
    "    estimator = Estimator()\n",
    "    estimator.set_options(method='statevector')\n",
    "    estimator.set_options(device=device)\n",
    "    estimator.set_options(seed=seed)\n",
    "    estimator.options\n",
    "\n",
    "print(f'Selected device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "epochs = 200\n",
    "\n",
    "start = time.time()\n",
    "for i in range(0, epochs):\n",
    "    print('iteration: ', i)\n",
    "    display(res)\n",
    "    pertubated_initial_point = initial_point + np.random.uniform(-0.1, 0.1, ansatz.num_parameters)\n",
    "    \n",
    "    qnn = EstimatorQNN(\n",
    "        circuit=qc,\n",
    "        estimator=Estimator(),\n",
    "        observables=GLOBAL_OPERATOR,\n",
    "        input_params=feature_map.parameters,\n",
    "        weight_params=ansatz.parameters\n",
    "    )\n",
    "\n",
    "    model = TorchConnector(qnn, initial_weights=pertubated_initial_point)\n",
    "\n",
    "    loss_function = nn.L1Loss() #This is MAE loss\n",
    "    optimizer = torch.optim.NAdam(model.parameters(), lr=0.05)\n",
    "\n",
    "    loss, weight = sampleWeightLoss(\n",
    "        model, \n",
    "        X_train_t,\n",
    "        y_train_t,\n",
    "        optimizer = optimizer, \n",
    "        loss_function = loss_function\n",
    "        )\n",
    "    res = pd.concat([res, pd.DataFrame(np.append(weight.numpy(), loss.numpy())).transpose()], ignore_index=True) \n",
    "    clear_output(wait=True)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "pd.DataFrame(res).astype('float').to_csv(f'{LOG_PATH}/{METHOD_TAG}/LossFunctionSurface.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print run statistics\n",
    "print(f'{device} Fit\\n'+\n",
    "      f'\\tQubits:\\t\\t{len(ansatz.qubits)}\\n'+\n",
    "      f'\\tId Blocks:\\t\\t{MAX_IDENTITIES_BLOCKS}\\n'+\n",
    "      f'\\tWeights:\\t{len(ansatz.parameters)}\\n'+\n",
    "      f'\\tEpochs:\\t\\t{epochs})\\n'+\n",
    "      f'\\tTime:\\t\\t{elapsed:0.2f} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure Training Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dataset (only the first two args)\n",
    "for x, y_target in zip(X_train_t, y_train_t):\n",
    "    if y_target == 1:\n",
    "        plt.plot(x[0], x[1], color=\"blue\", marker=\".\", markersize=5)\n",
    "    else:\n",
    "        plt.plot(x[0], x[1], color=\"green\", marker=\".\", markersize=5)\n",
    "plt.xlabel('PC 0')\n",
    "plt.ylabel('PC 1')\n",
    "plt.show()\n",
    "\n",
    "rows_b = np.where(np.array(y_train_t)==1)\n",
    "rows_g = np.where(np.array(y_train_t)==-1)\n",
    "print('Blue:', y_train_t[rows_b].size())\n",
    "print('Green:', y_train_t[rows_g].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dataset (only the first and the third arg)\n",
    "for x, y_target in zip(X_train_t, y_train_t):\n",
    "    if y_target == 1:\n",
    "        plt.plot(x[0], x[2], color=\"blue\", marker=\".\", markersize=5)\n",
    "    else:\n",
    "        plt.plot(x[0], x[2], color=\"green\", marker=\".\", markersize=5)\n",
    "plt.xlabel('PC 0')\n",
    "plt.ylabel('PC 2')\n",
    "plt.show()\n",
    "\n",
    "rows_b = np.where(np.array(y_train_t)==1)\n",
    "rows_g = np.where(np.array(y_train_t)==-1)\n",
    "print('Blue:', y_train_t[rows_b].size())\n",
    "print('Green:', y_train_t[rows_g].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = featureMapGenerator(FEATURE_DIM)\n",
    "identity_block = preTrainedBlockGenerator(MAX_QUBITS_CLASSIFICATION, MAX_IDENTITIES_BLOCKS, overlay=IDENTITY_BLOCKS_OVERLAY, entanglement=ENTANGLEMENT, insert_barriers=True)\n",
    "ansatz = identity_block['circuit']\n",
    "qc=circuitBuilder(feature_map, ansatz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To execute on multiple machines concurrently\n",
    "# Select separate sub-lists of instance numbers and run\n",
    "# Alternatively use range(MAX_INST) for all\n",
    "\n",
    "m = METHOD_TAG\n",
    "path = LOG_PATH\n",
    "times = []\n",
    "\n",
    "# for i in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]: \n",
    "for i in [6, 7]: \n",
    "    \n",
    "    # Mark that the method's data has been saved\n",
    "    if (exists(f'{path}/{m}')):\n",
    "        print(f'Adding a new instance {i} of method {m}\\n')\n",
    "    else:\n",
    "        makedirs(f'{path}/{m}')\n",
    "        print(f'Creating the first instance {i} of method {m}\\n')\n",
    "\n",
    "    m3_variances = sampleM3Var()\n",
    "        \n",
    "    # The identity block returns parameters in order different to that in the ansatz\n",
    "    identity_block = preTrainedBlockGenerator(MAX_QUBITS_CLASSIFICATION, MAX_IDENTITIES_BLOCKS, overlay=IDENTITY_BLOCKS_OVERLAY, entanglement=ENTANGLEMENT, insert_barriers=True)\n",
    "    id_dict = {k.name : v for k, v in identity_block['params_values'].items()}\n",
    "    initial_point = [id_dict[p.name] for p in list(ansatz.parameters)]\n",
    "\n",
    "    # By default this will run as a local simulation\n",
    "    qnn = EstimatorQNN(\n",
    "        circuit=qc,\n",
    "        estimator=Estimator(),\n",
    "        observables=GLOBAL_OPERATOR,\n",
    "        input_params=feature_map.parameters,\n",
    "        weight_params=ansatz.parameters\n",
    "    )\n",
    "\n",
    "    model = TorchConnector(qnn, initial_weights=initial_point)\n",
    "\n",
    "    loss_function = nn.L1Loss() # MSELoss()\n",
    "    optimizer = torch.optim.NAdam(model.parameters(), lr=0.05)\n",
    "\n",
    "    start = time.time()\n",
    "    model, losses, accuracy_train, accuracy_test, weights = train(\n",
    "        model, \n",
    "        MAX_ITER, \n",
    "        X_train_t,\n",
    "        y_train_t,\n",
    "        X_val_t,\n",
    "        y_val_t,\n",
    "        optimizer = optimizer, \n",
    "        loss_function = loss_function\n",
    "        )\n",
    "    elapsed = time.time() - start\n",
    "    times.append(elapsed)\n",
    "\n",
    "    pd.DataFrame(m3_variances, num_qubits).astype('float').to_csv(f'{path}/{m}/{m}-{i}-Variances.csv')\n",
    "    pd.DataFrame(losses).astype('float').to_csv(f'{path}/{m}/{m}-{i}-LossFunction.csv')\n",
    "    pd.DataFrame(accuracy_train).astype('float').to_csv(f'{path}/{m}/{m}-{i}-Scores-Train.csv')\n",
    "    pd.DataFrame(accuracy_test).astype('float').to_csv(f'{path}/{m}/{m}-{i}-Scores-Test.csv')\n",
    "    pd.DataFrame(weights).astype('float').to_csv(f'{path}/{m}/{m}-{i}-Weights.csv')\n",
    "    \n",
    "    # Mark that the method's data has been saved\n",
    "    if (exists(f'{path}/{m}/{m}-Method.csv')):\n",
    "        f = open(f'{path}/{m}/{m}-Method.csv', 'a')\n",
    "    else:\n",
    "        f = open(f'{path}/{m}/{m}-Method.csv', 'w')\n",
    "        f.write(f'{m},Instance,Max Inst,Examples,Features,Iterations\\n')\n",
    "    f.write(f',{i},{MAX_INST},{DATA_SIZE},{FEATURE_DIM},{MAX_ITER}\\n')\n",
    "    f.close()\n",
    "    \n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nAverage time / instance: {np.average(times)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "\n",
    "# Plot the loss of the last saved instance\n",
    "plot_method_data([pd.DataFrame(losses).astype('float').T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss of the last saved instance\n",
    "plot_method_data([pd.DataFrame(accuracy_test).astype('float').T],\n",
    "                title='Test Accuracy', dlabel='inst#', xlabel='Accuraccy', ylabel='Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep qiskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6a13c8f5b783206a43b73a673b4c249a5e5ee0a2cf865a54b80fb656bbdf8626"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
