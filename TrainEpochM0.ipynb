{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Variational Quantum Models with Barren Plateaus Mitigation Strategies\n",
    "*Training QNN treated with method 0 (General) of the BP mitigation strategy (Cancer)*\n",
    "\n",
    "**Authors:**\n",
    "- Jacob Cybulski and Thanh Nguyen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3839/561800655.py:10: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from qiskit.providers.fake_provider import FakeAlmadenV2, FakeSherbrooke\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "from qiskit.primitives import Estimator, BackendEstimator\n",
    "from qiskit_algorithms.gradients import ParamShiftEstimatorGradient\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from Components.train import *\n",
    "from Components.data import cancer_data\n",
    "from Components.circuits import *\n",
    "from Components.gradients import *\n",
    "from Components.utils import *\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "from os import makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global config for all experiments (data size, feature dim etc.) is stored here\n",
    "from GLOBAL_CONFIG import *\n",
    "# Remember to tag the method\n",
    "METHOD_TAG = 'm0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend = FakeSherbrooke()\n",
    "# estimator = BackendEstimator(backend)\n",
    "estimator = Estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 398 samples\n",
      "Testing set: 171 samples\n",
      "Number of features: 4\n",
      "PCA Explained variance: [4.43782605e+05 7.31010006e+03 7.03833742e+02 5.46487379e+01]\n",
      "Classes:[0 1]; Encoded as: [-1  1]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = cancer_data(PCA_n = FEATURE_DIM)\n",
    "# X_train, X_val, y_train, y_val = fetch_mnist(PCA_n = FEATURE_DIM, data_size=DATA_SIZE)\n",
    "# X_train, X_val, y_train, y_val = iris(pd=False, PCA_n=None)\n",
    "\n",
    "X_train_t = torch.from_numpy(X_train).to(torch.float32)\n",
    "y_train_t = torch.from_numpy(y_train).to(torch.float32)\n",
    "X_val_t = torch.from_numpy(X_val).to(torch.float32)\n",
    "y_val_t = torch.from_numpy(y_val).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found save folder: ./Logs-Cancer-v4/m0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if (exists(f'{LOG_PATH}/{METHOD_TAG}')):\n",
    "    print(f'Found save folder: {LOG_PATH}/{METHOD_TAG}\\n')\n",
    "else:\n",
    "    makedirs(f'{LOG_PATH}/{METHOD_TAG}')\n",
    "    print(f'Creating save folder: {LOG_PATH}/{METHOD_TAG}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Gradient Variance Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use in the fist phase\n",
    "\n",
    "num_qubits = list(range(2, MAX_QUBITS))\n",
    "\n",
    "# Globak operator for all ansatzes, measure all qubits\n",
    "G_O = [SparsePauliOp.from_list([('Z'*n, 1)]) for n in num_qubits]\n",
    "\n",
    "# Local operator for all ansatzes, measere 2 last qubits\n",
    "L_O = [SparsePauliOp.from_list([('I' * (n - 2)+'Z'*2, 1)]) for n in num_qubits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = num_qubits\n",
    "ansatzes_m0 = [AnsatzGenerator(n, r) for n, r in zip(num_qubits, reps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleM0Var():\n",
    "    gradients_m0 = sampleAnsatz(estimator, ansatzes_m0, G_O)\n",
    "    variance = getVariance(gradients_m0, num_qubits)\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = featureMapGenerator(FEATURE_DIM)\n",
    "ansatz = AnsatzGenerator(MAX_QUBITS_CLASSIFICATION, MAX_REPS, ENTANGLEMENT)\n",
    "qc = circuitBuilder(feature_map, ansatz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CPU'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Find what devices are available\n",
    "from qiskit_aer.backends import AerSimulator\n",
    "devices = AerSimulator().available_devices()\n",
    "devices\n",
    "\n",
    "# Force CPU - PyTorch+Qiskit too slow with GPU\n",
    "devices = ('CPU')\n",
    "devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: CPU\n"
     ]
    }
   ],
   "source": [
    "from qiskit.primitives import Estimator # For \"CPU\", ignores device=\"GPU\" option\n",
    "from qiskit_aer.primitives import Estimator as AerEstimator # For device=\"GPU\" option\n",
    "\n",
    "seed = 2023\n",
    "\n",
    "# Use GPU when present, otherwise CPU\n",
    "if 'GPU' in devices:\n",
    "    device = 'GPU'\n",
    "    estimator = AerEstimator(\n",
    "        backend_options={'seed_simulator': seed, 'method': 'statevector', \n",
    "                         'device' : device, 'cuStateVec_enable' : True},\n",
    "        run_options={'seed': seed, 'shots': 1000},\n",
    "        transpile_options={\"seed_transpiler\": seed},\n",
    "    )\n",
    "else:\n",
    "    device = 'CPU'\n",
    "    estimator = Estimator()\n",
    "    estimator.set_options(method='statevector')\n",
    "    estimator.set_options(device=device)\n",
    "    estimator.set_options(seed=seed)\n",
    "    estimator.options\n",
    "\n",
    "print(f'Selected device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  199\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.016843</td>\n",
       "      <td>-0.502266</td>\n",
       "      <td>-1.333053</td>\n",
       "      <td>-0.889038</td>\n",
       "      <td>-0.026498</td>\n",
       "      <td>2.511096</td>\n",
       "      <td>-2.276320</td>\n",
       "      <td>2.179116</td>\n",
       "      <td>3.048799</td>\n",
       "      <td>0.643568</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.226020</td>\n",
       "      <td>1.879604</td>\n",
       "      <td>-1.854540</td>\n",
       "      <td>2.766629</td>\n",
       "      <td>-0.212091</td>\n",
       "      <td>-1.689827</td>\n",
       "      <td>-2.162171</td>\n",
       "      <td>3.112765</td>\n",
       "      <td>-0.379081</td>\n",
       "      <td>1.006735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.822657</td>\n",
       "      <td>2.659216</td>\n",
       "      <td>2.467754</td>\n",
       "      <td>-2.331319</td>\n",
       "      <td>-2.251346</td>\n",
       "      <td>-0.496929</td>\n",
       "      <td>2.224321</td>\n",
       "      <td>-1.048311</td>\n",
       "      <td>-1.937582</td>\n",
       "      <td>1.543296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071902</td>\n",
       "      <td>1.615611</td>\n",
       "      <td>-2.147094</td>\n",
       "      <td>2.525911</td>\n",
       "      <td>0.810740</td>\n",
       "      <td>-0.607014</td>\n",
       "      <td>3.086781</td>\n",
       "      <td>-0.873671</td>\n",
       "      <td>0.916934</td>\n",
       "      <td>0.971047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.390072</td>\n",
       "      <td>1.917802</td>\n",
       "      <td>2.261895</td>\n",
       "      <td>0.828979</td>\n",
       "      <td>2.540658</td>\n",
       "      <td>-0.290711</td>\n",
       "      <td>2.236607</td>\n",
       "      <td>0.633590</td>\n",
       "      <td>-3.095434</td>\n",
       "      <td>2.032945</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.569831</td>\n",
       "      <td>-1.178735</td>\n",
       "      <td>-0.757146</td>\n",
       "      <td>-0.075716</td>\n",
       "      <td>0.344359</td>\n",
       "      <td>2.830569</td>\n",
       "      <td>-0.223886</td>\n",
       "      <td>-1.257615</td>\n",
       "      <td>-2.931905</td>\n",
       "      <td>1.012114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.116856</td>\n",
       "      <td>-1.107261</td>\n",
       "      <td>1.202920</td>\n",
       "      <td>-0.193918</td>\n",
       "      <td>1.109083</td>\n",
       "      <td>-2.123126</td>\n",
       "      <td>0.992474</td>\n",
       "      <td>1.842121</td>\n",
       "      <td>0.349893</td>\n",
       "      <td>-2.850870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786405</td>\n",
       "      <td>2.386807</td>\n",
       "      <td>-0.236290</td>\n",
       "      <td>-1.508756</td>\n",
       "      <td>2.762070</td>\n",
       "      <td>1.063613</td>\n",
       "      <td>-0.503251</td>\n",
       "      <td>-0.786952</td>\n",
       "      <td>-0.295652</td>\n",
       "      <td>0.953919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.940815</td>\n",
       "      <td>2.205239</td>\n",
       "      <td>2.257654</td>\n",
       "      <td>-1.693255</td>\n",
       "      <td>0.028384</td>\n",
       "      <td>2.927854</td>\n",
       "      <td>-0.404743</td>\n",
       "      <td>0.602938</td>\n",
       "      <td>-1.765269</td>\n",
       "      <td>-2.322556</td>\n",
       "      <td>...</td>\n",
       "      <td>1.754340</td>\n",
       "      <td>0.370583</td>\n",
       "      <td>-0.978450</td>\n",
       "      <td>-2.001925</td>\n",
       "      <td>-3.123446</td>\n",
       "      <td>3.068341</td>\n",
       "      <td>2.652854</td>\n",
       "      <td>-2.695330</td>\n",
       "      <td>2.808521</td>\n",
       "      <td>0.987096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>-0.057887</td>\n",
       "      <td>2.490746</td>\n",
       "      <td>0.651915</td>\n",
       "      <td>-2.953833</td>\n",
       "      <td>1.239401</td>\n",
       "      <td>0.950468</td>\n",
       "      <td>-2.788013</td>\n",
       "      <td>1.294825</td>\n",
       "      <td>-1.967174</td>\n",
       "      <td>-0.429437</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.962566</td>\n",
       "      <td>-2.913051</td>\n",
       "      <td>-0.815566</td>\n",
       "      <td>0.586766</td>\n",
       "      <td>0.160311</td>\n",
       "      <td>-1.962566</td>\n",
       "      <td>2.330389</td>\n",
       "      <td>-2.113026</td>\n",
       "      <td>1.877172</td>\n",
       "      <td>1.001769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>-0.990747</td>\n",
       "      <td>-1.432170</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1.629155</td>\n",
       "      <td>0.933517</td>\n",
       "      <td>3.054941</td>\n",
       "      <td>-1.421993</td>\n",
       "      <td>0.482080</td>\n",
       "      <td>2.164615</td>\n",
       "      <td>-1.551991</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.573950</td>\n",
       "      <td>-0.003642</td>\n",
       "      <td>2.069396</td>\n",
       "      <td>-2.616217</td>\n",
       "      <td>0.926728</td>\n",
       "      <td>0.742627</td>\n",
       "      <td>1.697308</td>\n",
       "      <td>-2.710376</td>\n",
       "      <td>-0.783600</td>\n",
       "      <td>1.016717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>-2.068624</td>\n",
       "      <td>-0.930616</td>\n",
       "      <td>1.115779</td>\n",
       "      <td>-1.102691</td>\n",
       "      <td>-0.842709</td>\n",
       "      <td>-2.309612</td>\n",
       "      <td>0.117068</td>\n",
       "      <td>-1.865221</td>\n",
       "      <td>-1.116396</td>\n",
       "      <td>1.874704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033227</td>\n",
       "      <td>-1.041817</td>\n",
       "      <td>-3.051211</td>\n",
       "      <td>-0.625449</td>\n",
       "      <td>1.449066</td>\n",
       "      <td>-0.642373</td>\n",
       "      <td>2.379999</td>\n",
       "      <td>1.272285</td>\n",
       "      <td>1.808455</td>\n",
       "      <td>0.986685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2.163589</td>\n",
       "      <td>1.799700</td>\n",
       "      <td>-2.583833</td>\n",
       "      <td>-3.090748</td>\n",
       "      <td>0.429503</td>\n",
       "      <td>0.143951</td>\n",
       "      <td>2.609350</td>\n",
       "      <td>2.283876</td>\n",
       "      <td>0.921899</td>\n",
       "      <td>2.659124</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.266744</td>\n",
       "      <td>2.233055</td>\n",
       "      <td>-0.299866</td>\n",
       "      <td>0.524244</td>\n",
       "      <td>-1.511547</td>\n",
       "      <td>-2.738013</td>\n",
       "      <td>-1.774081</td>\n",
       "      <td>1.699965</td>\n",
       "      <td>2.480792</td>\n",
       "      <td>0.984667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-0.853683</td>\n",
       "      <td>-0.929651</td>\n",
       "      <td>2.018631</td>\n",
       "      <td>-2.397667</td>\n",
       "      <td>1.345047</td>\n",
       "      <td>-2.863437</td>\n",
       "      <td>2.346188</td>\n",
       "      <td>-2.964967</td>\n",
       "      <td>2.166457</td>\n",
       "      <td>-2.643832</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.826833</td>\n",
       "      <td>1.381797</td>\n",
       "      <td>1.146458</td>\n",
       "      <td>0.712013</td>\n",
       "      <td>-0.219504</td>\n",
       "      <td>0.004663</td>\n",
       "      <td>-0.550440</td>\n",
       "      <td>-2.144651</td>\n",
       "      <td>0.472933</td>\n",
       "      <td>1.007511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows Ã— 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0   -2.016843 -0.502266 -1.333053 -0.889038 -0.026498  2.511096 -2.276320   \n",
       "1    2.822657  2.659216  2.467754 -2.331319 -2.251346 -0.496929  2.224321   \n",
       "2    0.390072  1.917802  2.261895  0.828979  2.540658 -0.290711  2.236607   \n",
       "3    3.116856 -1.107261  1.202920 -0.193918  1.109083 -2.123126  0.992474   \n",
       "4    2.940815  2.205239  2.257654 -1.693255  0.028384  2.927854 -0.404743   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "194 -0.057887  2.490746  0.651915 -2.953833  1.239401  0.950468 -2.788013   \n",
       "195 -0.990747 -1.432170  0.000016  1.629155  0.933517  3.054941 -1.421993   \n",
       "196 -2.068624 -0.930616  1.115779 -1.102691 -0.842709 -2.309612  0.117068   \n",
       "197  2.163589  1.799700 -2.583833 -3.090748  0.429503  0.143951  2.609350   \n",
       "198 -0.853683 -0.929651  2.018631 -2.397667  1.345047 -2.863437  2.346188   \n",
       "\n",
       "          7         8         9    ...       95        96        97   \\\n",
       "0    2.179116  3.048799  0.643568  ... -0.226020  1.879604 -1.854540   \n",
       "1   -1.048311 -1.937582  1.543296  ...  0.071902  1.615611 -2.147094   \n",
       "2    0.633590 -3.095434  2.032945  ... -1.569831 -1.178735 -0.757146   \n",
       "3    1.842121  0.349893 -2.850870  ...  0.786405  2.386807 -0.236290   \n",
       "4    0.602938 -1.765269 -2.322556  ...  1.754340  0.370583 -0.978450   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "194  1.294825 -1.967174 -0.429437  ... -0.962566 -2.913051 -0.815566   \n",
       "195  0.482080  2.164615 -1.551991  ... -1.573950 -0.003642  2.069396   \n",
       "196 -1.865221 -1.116396  1.874704  ...  0.033227 -1.041817 -3.051211   \n",
       "197  2.283876  0.921899  2.659124  ... -2.266744  2.233055 -0.299866   \n",
       "198 -2.964967  2.166457 -2.643832  ... -2.826833  1.381797  1.146458   \n",
       "\n",
       "          98        99        100       101       102       103       104  \n",
       "0    2.766629 -0.212091 -1.689827 -2.162171  3.112765 -0.379081  1.006735  \n",
       "1    2.525911  0.810740 -0.607014  3.086781 -0.873671  0.916934  0.971047  \n",
       "2   -0.075716  0.344359  2.830569 -0.223886 -1.257615 -2.931905  1.012114  \n",
       "3   -1.508756  2.762070  1.063613 -0.503251 -0.786952 -0.295652  0.953919  \n",
       "4   -2.001925 -3.123446  3.068341  2.652854 -2.695330  2.808521  0.987096  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "194  0.586766  0.160311 -1.962566  2.330389 -2.113026  1.877172  1.001769  \n",
       "195 -2.616217  0.926728  0.742627  1.697308 -2.710376 -0.783600  1.016717  \n",
       "196 -0.625449  1.449066 -0.642373  2.379999  1.272285  1.808455  0.986685  \n",
       "197  0.524244 -1.511547 -2.738013 -1.774081  1.699965  2.480792  0.984667  \n",
       "198  0.712013 -0.219504  0.004663 -0.550440 -2.144651  0.472933  1.007511  \n",
       "\n",
       "[199 rows x 105 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = pd.DataFrame()\n",
    "\n",
    "start = time.time()\n",
    "for i in range(0, 200):\n",
    "    print('iteration: ', i)\n",
    "    display(res)\n",
    "    point = np.random.uniform(-np.pi, np.pi, ansatz.num_parameters)\n",
    "    \n",
    "    qnn = EstimatorQNN(\n",
    "        circuit=qc,\n",
    "        estimator=estimator,\n",
    "        observables=GLOBAL_OPERATOR,\n",
    "        input_params=feature_map.parameters,\n",
    "        weight_params=ansatz.parameters\n",
    "    )\n",
    "\n",
    "    model = TorchConnector(qnn, initial_weights=point)\n",
    "\n",
    "    loss_function = nn.L1Loss() #This is MAE loss\n",
    "    optimizer = torch.optim.NAdam(model.parameters(), lr=0.05)\n",
    "\n",
    "    loss, weight = sampleWeightLoss(\n",
    "        model, \n",
    "        X_train_t,\n",
    "        y_train_t,\n",
    "        optimizer = optimizer, \n",
    "        loss_function = loss_function\n",
    "        )\n",
    "    res = pd.concat([res, pd.DataFrame(np.append(weight.numpy(), loss.numpy())).transpose()], ignore_index=True) \n",
    "    clear_output(wait=True)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "pd.DataFrame(res).astype('float').to_csv(f'{LOG_PATH}/{METHOD_TAG}/LossFunctionSurface.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print run statistics\n",
    "print(f'{device} Fit\\n'+\n",
    "      f'\\tQubits:\\t\\t{num_qubits}\\n'+\n",
    "      f'\\tReps:\\t\\t{reps}\\n'+\n",
    "      f'\\tWeights:\\t{len(ansatz.parameters)}\\n'+\n",
    "      f'\\tEpochs:\\t\\t{epochs})\\n'+\n",
    "      f'\\tTime:\\t\\t{elapsed:0.2f} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure Training Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dataset (only the first two args)\n",
    "for x, y_target in zip(X_train_t, y_train_t):\n",
    "    if y_target == 1:\n",
    "        plt.plot(x[0], x[1], color=\"blue\", marker=\".\", markersize=5)\n",
    "    else:\n",
    "        plt.plot(x[0], x[1], color=\"green\", marker=\".\", markersize=5)\n",
    "plt.xlabel('PC 0')\n",
    "plt.ylabel('PC 1')\n",
    "plt.show()\n",
    "\n",
    "rows_b = np.where(np.array(y_train_t)==1)\n",
    "rows_g = np.where(np.array(y_train_t)==-1)\n",
    "print('Blue:', y_train_t[rows_b].size())\n",
    "print('Green:', y_train_t[rows_g].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dataset (only the first and the third arg)\n",
    "for x, y_target in zip(X_train_t, y_train_t):\n",
    "    if y_target == 1:\n",
    "        plt.plot(x[0], x[2], color=\"blue\", marker=\".\", markersize=5)\n",
    "    else:\n",
    "        plt.plot(x[0], x[2], color=\"green\", marker=\".\", markersize=5)\n",
    "plt.xlabel('PC 0')\n",
    "plt.ylabel('PC 2')\n",
    "plt.show()\n",
    "\n",
    "rows_b = np.where(np.array(y_train_t)==1)\n",
    "rows_g = np.where(np.array(y_train_t)==-1)\n",
    "print('Blue:', y_train_t[rows_b].size())\n",
    "print('Green:', y_train_t[rows_g].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = featureMapGenerator(FEATURE_DIM)\n",
    "ansatz = AnsatzGenerator(MAX_QUBITS_CLASSIFICATION, MAX_REPS, ENTANGLEMENT)\n",
    "qc = circuitBuilder(feature_map, ansatz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To execute on multiple machines concurrently\n",
    "# Select separate sub-lists of instance numbers and run\n",
    "# Alternatively use range(MAX_INST) for all\n",
    "\n",
    "m = METHOD_TAG\n",
    "path = LOG_PATH\n",
    "times = []\n",
    "\n",
    "# for i in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]: \n",
    "for i in [0, 1]: \n",
    "     \n",
    "    # Mark that the method's data has been saved\n",
    "    if (exists(f'{path}/{m}')):\n",
    "        print(f'Adding a new instance {i} of method {m}\\n')\n",
    "    else:\n",
    "        makedirs(f'{path}/{m}')\n",
    "        print(f'Creating the first instance {i} of method {m}\\n')\n",
    "\n",
    "    m0_variances = sampleM0Var()\n",
    "        \n",
    "    # By default this will run as a local simulation\n",
    "    qnn = EstimatorQNN(\n",
    "        circuit=qc,\n",
    "        estimator=estimator,\n",
    "        observables=GLOBAL_OPERATOR,\n",
    "        input_params=feature_map.parameters,\n",
    "        weight_params=ansatz.parameters\n",
    "    )\n",
    "\n",
    "    # By default random initialisation\n",
    "    model = TorchConnector(qnn)\n",
    "\n",
    "    loss_function = nn.L1Loss() # nn.MSELoss()\n",
    "    optimizer = torch.optim.NAdam(model.parameters(), lr=0.05)\n",
    "\n",
    "    start = time.time()\n",
    "    model, losses, accuracy_train, accuracy_test, weights = train(\n",
    "        model, \n",
    "        MAX_ITER, \n",
    "        X_train_t,\n",
    "        y_train_t,\n",
    "        X_val_t,\n",
    "        y_val_t,\n",
    "        optimizer = optimizer, \n",
    "        loss_function = loss_function\n",
    "        )\n",
    "    elapsed = time.time() - start\n",
    "    times.append(elapsed)\n",
    "\n",
    "    pd.DataFrame(m0_variances, num_qubits).astype('float').to_csv(f'{path}/{m}/{m}-{i}-Variances.csv')\n",
    "    pd.DataFrame(losses).astype('float').to_csv(f'{path}/{m}/{m}-{i}-LossFunction.csv')\n",
    "    pd.DataFrame(accuracy_train).astype('float').to_csv(f'{path}/{m}/{m}-{i}-Scores-Train.csv')\n",
    "    pd.DataFrame(accuracy_test).astype('float').to_csv(f'{path}/{m}/{m}-{i}-Scores-Test.csv')\n",
    "    pd.DataFrame(weights).astype('float').to_csv(f'{path}/{m}/{m}-{i}-Weights.csv')\n",
    "    \n",
    "    # Mark that the method's data has been saved\n",
    "    if (exists(f'{path}/{m}/{m}-Method.csv')):\n",
    "        f = open(f'{path}/{m}/{m}-Method.csv', 'a')\n",
    "    else:\n",
    "        f = open(f'{path}/{m}/{m}-Method.csv', 'w')\n",
    "        f.write(f'{m},Instance,Max Inst,Examples,Features,Iterations\\n')\n",
    "    f.write(f',{i},{MAX_INST},{DATA_SIZE},{FEATURE_DIM},{MAX_ITER}\\n')\n",
    "    f.close()\n",
    "    \n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nAverage time / instance: {np.average(times)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "\n",
    "# Plot the loss of the last saved instance\n",
    "plot_method_data([pd.DataFrame(losses).astype('float').T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss of the last saved instance\n",
    "plot_method_data([pd.DataFrame(accuracy_test).astype('float').T],\n",
    "                title='Test Accuracy', dlabel='inst#', xlabel='Accuraccy', ylabel='Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep qiskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "6a13c8f5b783206a43b73a673b4c249a5e5ee0a2cf865a54b80fb656bbdf8626"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
